{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3da0cda",
   "metadata": {},
   "source": [
    "### Importing the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "trees = []\n",
    "path = r'C:\\Users\\Danyal\\Desktop\\DCU\\Mechanics of Search\\CA6005_2022-assignment\\CA6005_2022\\COLLECTION'\n",
    "for filename in os.listdir(path):\n",
    "    if not filename.endswith('.xml'): continue\n",
    "    fullname = os.path.join(path, filename)\n",
    "    trees.append(ET.parse(fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = []\n",
    "for tree in trees:\n",
    "    roots.append(tree.getroot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca887530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root1=roots[2].findall(\"TEXT\")\n",
    "for text in roots[1].findall(\"TEXT\"):\n",
    "    print(text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b780b3",
   "metadata": {},
   "source": [
    "### Making Three seperate lists for the Text, Headline and the Doc id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "headings = []\n",
    "doc_id = [] \n",
    "\n",
    "for root in roots:\n",
    "    for text in root.findall(\"TEXT\"):\n",
    "        texts.append(text.text)\n",
    "    for head in root.findall(\"HEADLINE\"):\n",
    "        headings.append(head.text)\n",
    "    for d_id in root.findall(\"DOCID\"):\n",
    "        doc_id.append(d_id.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b33ffab",
   "metadata": {},
   "source": [
    "### Creating a dataframe with three key values: Text, Headline and the Doc id\n",
    "#### These three keys will be containing the lists with the data. The length of each list will be the total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e24cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = {'document_id':doc_id, 'headlines':headings, 'text':texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e569fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe['text'][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe['text'][10]\n",
    "def gettext(index):\n",
    "    return dataframe['text'][index]\n",
    "\n",
    "def gethead (index):\n",
    "    return dataframe['headlines'][index]\n",
    "\n",
    "def getid (index):\n",
    "    return dataframe['document_id'][index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a764d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gettext(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141fd75",
   "metadata": {},
   "source": [
    "### Repeating the same steps for the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e352c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrees = []\n",
    "path = r'C:\\Users\\Danyal\\Desktop\\DCU\\Mechanics of Search\\CA6005_2022-assignment\\CA6005_2022\\topics\\topics'\n",
    "for topic in os.listdir(path):\n",
    "    if not topic.endswith('.xml'): continue\n",
    "    qfullname = os.path.join(path, topic)\n",
    "    qtrees.append(ET.parse(qfullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "qroots = []\n",
    "for qtree in qtrees:\n",
    "    qroots.append(qtree.getroot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = []\n",
    "title = []\n",
    "desc = [] \n",
    "narr = [] \n",
    "\n",
    "for qroot in qroots:\n",
    "    for query_id in qroot.findall(\"QUERYID\"):\n",
    "        q_id.append(query_id.text)\n",
    "    for t in qroot.findall(\"TITLE\"):\n",
    "        title.append(t.text)\n",
    "    for d in qroot.findall(\"DESC\"):\n",
    "        desc.append(d.text)\n",
    "    for n in qroot.findall(\"NARR\"):\n",
    "        narr.append(n.text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4299d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dataframe = {'query_id':q_id, 'title':title, 'description':desc, 'narration':narr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dataframe['description'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getq_id(index):\n",
    "    return q_dataframe['query_id'][index]\n",
    "\n",
    "def gettitle (index):\n",
    "    return q_dataframe['title'][index]\n",
    "\n",
    "def getdesc (index):\n",
    "    return q_dataframe['description'][index]\n",
    "\n",
    "def getnarr (index):\n",
    "    return q_dataframe['narration'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "getdesc(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6185df",
   "metadata": {},
   "source": [
    "### Creating a new dictionary for further processing (If something wrong happens we can always have a backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66372ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2b757",
   "metadata": {},
   "source": [
    "## Pre-Processing of the document text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0652a2",
   "metadata": {},
   "source": [
    "### Getting rid of the none-type and converting the text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b792b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_type_clean(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "while (p<len(data['headlines'])):\n",
    "    data['headlines'][p] = none_type_clean(data['headlines'][p]).lower() # = data['headlines'][p].lower()\n",
    "    p=p+1\n",
    "p = 0   \n",
    "while (p<len(data['text'])):\n",
    "    data['text'][p] = none_type_clean(data['text'][p]).lower() # = data['headlines'][p].lower()\n",
    "    p=p+1\n",
    "    \n",
    "# len(data['headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92cfdcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c444b0",
   "metadata": {},
   "source": [
    "### Getting rid of contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb30eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# All the english contractions\n",
    "\n",
    "contractions_dict = { \"ain \\' t\": \"are not\",\" \\' s\":\" is\",\"aren \\' t\": \"are not\",\"can \\' t\": \"can not\",\"can \\' t \\' ve\": \"cannot have\",\n",
    "\" \\' cause\": \"because\",\"could \\' ve\": \"could have\",\"couldn \\' t\": \"could not\",\"couldn \\' t \\' ve\": \"could not have\",\n",
    "\"didn \\' t\": \"did not\",\"doesn \\' t\": \"does not\",\"don \\' t\": \"do not\",\"hadn \\' t\": \"had not\",\"hadn \\' t \\' ve\": \"had not have\",\n",
    "\"hasn \\' t\": \"has not\",\"haven \\' t\": \"have not\",\"he \\' d\": \"he would\",\"he \\' d \\' ve\": \"he would have\",\"he \\' ll\": \"he will\",\n",
    "\"he \\' ll \\' ve\": \"he will have\",\"how \\' d\": \"how did\",\"how \\' d \\' y\": \"how do you\",\"how \\' ll\": \"how will\",\"i \\' d\": \"i would\",\n",
    "\"i \\' d \\' ve\": \"i would have\",\"i \\' ll\": \"i will\",\"i \\' ll \\' ve\": \"i will have\",\"i \\' m\": \"i am\",\"i \\' ve\": \"i have\",\n",
    "\"isn \\' t\": \"is not\",\"it \\' d\": \"it would\",\"it \\' d \\' ve\": \"it would have\",\"it \\' ll\": \"it will\",\"it \\' ll \\' ve\": \"it will have\",\n",
    "\"let \\' s\": \"let us\",\"ma \\' am\": \"madam\",\"mayn \\' t\": \"may not\",\"might \\' ve\": \"might have\",\"mightn \\' t\": \"might not\",\n",
    "\"mightn \\' t \\' ve\": \"might not have\",\"must \\' ve\": \"must have\",\"mustn \\' t\": \"must not\",\"mustn \\' t \\' ve\": \"must not have\",\n",
    "\"needn \\' t\": \"need not\",\"needn \\' t \\' ve\": \"need not have\",\"o \\' clock\": \"of the clock\",\"oughtn \\' t\": \"ought not\",\n",
    "\"oughtn \\' t \\' ve\": \"ought not have\",\"shan \\' t\": \"shall not\",\"sha \\' n \\' t\": \"shall not\",\n",
    "\"shan \\' t \\' ve\": \"shall not have\",\"she \\' d\": \"she would\",\"she \\' d \\' ve\": \"she would have\",\"she \\' ll\": \"she will\",\n",
    "\"she \\' ll \\' ve\": \"she will have\",\"should \\' ve\": \"should have\",\"shouldn \\' t\": \"should not\",\n",
    "\"shouldn \\' t \\' ve\": \"should not have\",\"so \\' ve\": \"so have\",\"that \\' d\": \"that would\",\"that \\' d \\' ve\": \"that would have\",\n",
    "\"there \\' d\": \"there would\",\"there \\' d \\' ve\": \"there would have\",\n",
    "\"they \\' d\": \"they would\",\"they \\' d \\' ve\": \"they would have\",\"they \\' ll\": \"they will\",\"they \\' ll \\' ve\": \"they will have\",\n",
    "\"they \\' re\": \"they are\",\"they \\' ve\": \"they have\",\"to \\' ve\": \"to have\",\"wasn \\' t\": \"was not\",\"we \\' d\": \"we would\",\n",
    "\"we \\' d \\' ve\": \"we would have\",\"we \\' ll\": \"we will\",\"we \\' ll \\' ve\": \"we will have\",\"we \\' re\": \"we are\",\"we \\' ve\": \"we have\",\n",
    "\"weren \\' t\": \"were not\",\"what \\' ll\": \"what will\",\"what \\' ll \\' ve\": \"what will have\",\"what \\' re\": \"what are\",\n",
    "\"what \\' ve\": \"what have\",\"when \\' ve\": \"when have\",\"where \\' d\": \"where did\",\n",
    "\"where \\' ve\": \"where have\",\"who \\' ll\": \"who will\",\"who \\' ll \\' ve\": \"who will have\",\"who \\' ve\": \"who have\",\n",
    "\"why \\' ve\": \"why have\",\"will \\' ve\": \"will have\",\"won \\' t\": \"will not\",\"won \\' t \\' ve\": \"will not have\",\n",
    "\"would \\' ve\": \"would have\",\"wouldn \\' t\": \"would not\",\"wouldn \\' t \\' ve\": \"would not have\",\"y \\' all\": \"you all\",\n",
    "\"y \\' all \\' d\": \"you all would\",\"y \\' all \\' d \\' ve\": \"you all would have\",\"y \\' all \\' re\": \"you all are\",\"y \\' all \\' ve\": \"you all have\",\n",
    "\"you \\' d\": \"you would\",\"you \\' d \\' ve\": \"you would have\",\"you \\' ll\": \"you will\",\"you \\' ll \\' ve\": \"you will have\",\n",
    "\"you \\' re\": \"you are\",\"you \\' ve\": \"you have\"}\n",
    "\n",
    "contractions_re=re.compile('(%s)' % \"|\".join(contractions_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0295fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to add space \n",
    "# def add_space(s):            \n",
    "#     pat = re.compile(r\"(['])\")\n",
    "#     return pat.sub(\" \\\\1 \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a214ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in contractions_dict:\n",
    "#     key1 = add_space(key)\n",
    "#     contractions_dict[key1] = contractions_dict.pop(key)\n",
    "#     print(contractions_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ff1a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expand_contractions(data['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['headlines']=training_corpus['cleaned'].apply(lambda x:expand_contractions(x))\n",
    "p = 0\n",
    "while (p<len(data['headlines'])):\n",
    "    data['headlines'][p] = expand_contractions(data['headlines'][p]) \n",
    "    p=p+1\n",
    "\n",
    "p = 0\n",
    "while (p<len(data['text'])):\n",
    "    data['text'][p] = expand_contractions(data['text'][p]) \n",
    "    p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2470d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88ffa0",
   "metadata": {},
   "source": [
    "### Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f3a37",
   "metadata": {},
   "source": [
    "##### Removing words with digits, replace newline characters with whitespace, remove URLs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fUNCTION TO CLEAN\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text=re.sub('\\w*\\d\\w*','', text)\n",
    "#     text=re.sub('\\n',' ',text)\n",
    "#     text=re.sub(r\"http\\S+\", \"\", text)\n",
    "#     text=re.sub('[^a-z]',' ',text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fccd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# while (p<len(data['headlines'])):\n",
    "#     data['headlines'][p] = clean_text(data['headlines'][p]) \n",
    "#     p=p+1\n",
    "\n",
    "# p = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     data['text'][p] = clean_text(data['text'][p]) \n",
    "#     p=p+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4844739",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punct(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "while (p<len(data['headlines'])):\n",
    "    data['headlines'][p] = remove_punct(data['headlines'][p]) \n",
    "    p=p+1\n",
    "\n",
    "p = 0\n",
    "while (p<len(data['text'])):\n",
    "    data['text'][p] = remove_punct(data['text'][p]) \n",
    "    p=p+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f4ff8",
   "metadata": {},
   "source": [
    "### Tokens and stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140aa090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ed0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download()\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c8faf",
   "metadata": {},
   "source": [
    "### POS taging and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map POS tag to first character lemmatize() accepts\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(text):\n",
    "    return [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "# lem(data['text'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d40c95",
   "metadata": {},
   "source": [
    "### Checking if it is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"my name is danyal\"\n",
    "print(lem(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0163c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "while (p<len(data['headlines'])):\n",
    "    data['headlines'][p] = lem(data['headlines'][p]) \n",
    "    p=p+1\n",
    "\n",
    "p = 0\n",
    "while (p<len(data['text'])):\n",
    "    data['text'][p] = lem(data['text'][p]) \n",
    "    p=p+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e4b46",
   "metadata": {},
   "source": [
    "### Storing the processed text in new variable as a chckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32432ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# a_file = open(\"data.csv\", \"w\")\n",
    "# # a_dict = {\"a\": 1, \"b\": 2}\n",
    "\n",
    "# writer = csv.writer(a_file)\n",
    "# for key, value in data.items():\n",
    "#     writer.writerow([key, value])\n",
    "\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad13b8c",
   "metadata": {},
   "source": [
    "### Creating tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# while (p<len(data['headlines'])):\n",
    "#     data['headlines'][p] = nltk.word_tokenize(data['headlines'][p])\n",
    "#     p=p+1\n",
    "\n",
    "# p = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     data['text'][p] = nltk.word_tokenize(data['text'][p])\n",
    "#     p=p+1\n",
    "    \n",
    "# data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff5bf5",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f804230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# while (p<len(data['headlines'])):\n",
    "#     data['headlines'][p] = [word for word in data['headlines'][p] if word not in stopwords] \n",
    "#     p=p+1\n",
    "\n",
    "# p = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     data['text'][p] = [word for word in data['text'][p] if word not in stopwords]\n",
    "#     p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4077ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc97095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a579180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stemmer(text):\n",
    "#     stemmed = [porter.stem(word) for word in text]\n",
    "#     return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a08b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer(data['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# while (p<len(data['headlines'])):\n",
    "#     data['headlines'][p] = stemmer(data['headlines'][p])\n",
    "#     p=p+1\n",
    "\n",
    "# p = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     data['text'][p] = stemmer(data['text'][p])\n",
    "#     p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eeb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# q = 0\n",
    "# while (p<len(data['headlines'])):\n",
    "#     while(q<len(data['headlines'][p])):\n",
    "#         data['headlines'][p][q] = stemmer(data['headlines'][p][q])\n",
    "#         q=q+1\n",
    "#     p=p+1\n",
    "\n",
    "# p = 0\n",
    "# q = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     while(q<len(data['text'][p])):\n",
    "#         data['text'][p][q] = stemmer(data['text'][p][q])\n",
    "#         q=q+1\n",
    "#     p=p+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428e834",
   "metadata": {},
   "source": [
    "### Pos Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# while (p<len(data['headlines'])):\n",
    "#     data['headlines'][p] = nltk.pos_tag(data['headlines'][p]) \n",
    "#     p=p+1\n",
    "\n",
    "# p = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     data['text'][p] = nltk.pos_tag(data['text'][p]) \n",
    "#     p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faeb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer(data['text'][2][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# q = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     while (q<len(data['text'][p])):\n",
    "#         data['text'][p][q][0] = lem(data['text'][p][q][0])\n",
    "#         q=q+1\n",
    "#     p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f394ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text'][2][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7176e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# while (p<len(data['headlines'])):\n",
    "#     data['headlines'][p] = nltk.pos_tag(data['headlines'][p]) \n",
    "#     p=p+1\n",
    "\n",
    "# p = 0\n",
    "# while (p<len(data['text'])):\n",
    "#     data['text'][p] = nltk.pos_tag(data['text'][p]) \n",
    "#     p=p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_xml_files(path):\n",
    "#     xml_list = []\n",
    "#     for filename in os.listdir(path):\n",
    "#         if filename.endswith(\".xml\"):\n",
    "#             xml_list.append(os.path.join(path, filename))\n",
    "#     return xml_list\n",
    "\n",
    "# get_xml_files(path)\n",
    "\n",
    "# from xml.dom import minidom\n",
    "\n",
    "# documents = []\n",
    "# for current_doc in get_xml_files(path):\n",
    "#     documents.append(minidom.parse(current_doc))\n",
    "# #file = minidom.parse('models.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c65b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = []\n",
    "# for doc in documents:\n",
    "#     texts.append(doc.getElementsByTagName('TEXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"you ' re\"\n",
    "# print(add_space(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.compile(r\"([.()!])\")\n",
    "# def rem_whitwspace_before(s):\n",
    "#     for p in range(0, len(s)-1):\n",
    "#         if s[p] == \"'\":\n",
    "# def add_space(s):            \n",
    "#     pat = re.compile(r\"(['])\")\n",
    "#     return pat.sub(\" \\\\1 \", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0fd632",
   "metadata": {},
   "source": [
    "### Installing dill and using it to save the session so that will not have to start from the begining \n",
    "#### The lemmatize function took approximately 3 hrs for the text in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c451edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09d47",
   "metadata": {},
   "source": [
    "### Using this command we can restore the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8463bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "while (p<len(data['headlines'])):\n",
    "    data['headlines'][p] = [word for word in data['headlines'][p] if word not in stopwords] \n",
    "    p=p+1\n",
    "\n",
    "p = 0\n",
    "while (p<len(data['text'])):\n",
    "    data['text'][p] = [word for word in data['text'][p] if word not in stopwords]\n",
    "    p=p+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52f92a",
   "metadata": {},
   "source": [
    "### Combining the headlines and the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf ={key: value for key, *value in zip(data['document_id'], data['headlines'], data['text'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d12347",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9530e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in dataf.keys():\n",
    "    dataf[i]=list(chain.from_iterable(dataf[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97915385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44503db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574398bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0281a",
   "metadata": {},
   "source": [
    "### Creating a list containing the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d052cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [items for items in dataf.values()]\n",
    "vocabulary = list(chain.from_iterable(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eaa0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e345c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the vocabulary of words we have in each of the doccuments.\n",
    "\n",
    "total_vocabulary = list(set(vocabulary))\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724baee",
   "metadata": {},
   "source": [
    "## Creating an inverted index:\n",
    "#### A dictionary containing terms as keys, lists of tuples containing the documents in which that term is present along with the frequency of that term in that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = {}\n",
    "for x in total_vocabulary:\n",
    "    for y,z in dataf.items():\n",
    "        if x in z:\n",
    "            word_freq = z.count(x)\n",
    "            inv_index.setdefault(x,[]).append((y,word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a066cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c10c33",
   "metadata": {},
   "source": [
    "### The inverted index took approximately 5.30 hrs to complete therefore storing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store inv_index\n",
    "%store dataf\n",
    "%store total_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ea572",
   "metadata": {},
   "source": [
    "#### Command for restoring the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r inv_index\n",
    "%store -r dataf\n",
    "%store -r total_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e96930",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index['icontroversy'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ee603",
   "metadata": {},
   "source": [
    "### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def querry_processing(querry_text):\n",
    "    \n",
    "    #Removing None_types from the querries.\n",
    "    querry_text=none_type_clean(querry_text)\n",
    "    \n",
    "    #Changing the case of the querry_text\n",
    "    querry_text=querry_text.lower()\n",
    "    \n",
    "    #Expanding english contractions\n",
    "    querry_text = expand_contractions(querry_text)\n",
    "    \n",
    "    #Removing Punctuations\n",
    "    querry_text = remove_punct(querry_text)\n",
    "    \n",
    "    #Lemmatisation, pos-tagging and tokenisation\n",
    "    querry_text = lem(querry_text)\n",
    "    \n",
    "    #Stop_words removal\n",
    "    #querry_text = [word for word in querry_text if not word in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "querry_text = [word for word in querry_text if not word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d23f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(q_dataframe['title'])):\n",
    "    q_dataframe['title'][i]=querry_processing(q_dataframe['title'][i])\n",
    "#     q_data['q_desc'][i]=querry_processing(q_data['q_desc'][i])\n",
    "#     q_data['q_narr'][i]=querry_processing(q_data['q_narr'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r q_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2102b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_data ={key: value for key, value in zip(q_dataframe['query_id'], q_dataframe['title'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bcc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db2c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bee49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qu = ['oil', 'accident',  'bird']\n",
    "# def vec_score(q,inv):\n",
    "#     vec = {}\n",
    "#     w_score = 0\n",
    "#     nume = 0\n",
    "#     doc_score = 0\n",
    "#     for word in q:\n",
    "#             for i in range(0, len(inv[word])):\n",
    "#                 vec[inv[word][i][0]] = (q.count(word) * inv[word][i][1])/(math.sqrt((inv[word][i][1])**2) * (math.sqrt((q.count(word))**2)) )\n",
    "# #                 w_score = w_score + doc_score\n",
    "# #             nume = nume+w_score\n",
    "#     return vec\n",
    "# len(vec_score(qu, inv_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "qu = ['oil', 'accident',  'bird']\n",
    "# def vec_score(q,inv):\n",
    "#     vec = {}\n",
    "#     vec1 = {}\n",
    "#     doc_list = []\n",
    "#     for word in q:\n",
    "#         for i in range (0, len(inv[word])):\n",
    "#             if inv[word][i][0] in doc_list:\n",
    "#                 vec[inv[word][i][0]] = vec[inv[word][i][0]] + (q.count(word) * inv[word][i][1])/(math.sqrt((inv[word][i][1])**2) * (math.sqrt((q.count(word))**2)))\n",
    "#                 vec1[inv[word][i][0]]\n",
    "#             if inv[word][i][0] not in doc_list:\n",
    "#                 doc_list.append(inv[word][i][0])\n",
    "            \n",
    "#     return vec\n",
    "\n",
    "# vec_score(qu, inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245476f",
   "metadata": {},
   "source": [
    "### Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_num (query):\n",
    "    vecn = {}\n",
    "    for term in query:\n",
    "        if term not in inv_index:\n",
    "            continue\n",
    "        for i in range (0, len(inv_index[term])):\n",
    "            if inv_index[term][i][0] not in vecn:\n",
    "                vecn[inv_index[term][i][0]] = 0\n",
    "            vecn[inv_index[term][i][0]] += inv_index[term][i][1] * query.count(term)\n",
    "    return vecn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823185e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(ls): \n",
    "    # initialize a null list\n",
    "    unique_l = []     \n",
    "    # traverse for all elements\n",
    "    for x in ls:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_l:\n",
    "            unique_l.append(x)\n",
    "    return unique_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecnu = {}\n",
    "for q_id in q_data:\n",
    "    vecnu[q_id] = vec_num(q_data[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "denom1 = {}\n",
    "t_d = []\n",
    "for did in dataf:\n",
    "    denom1[did] = 0\n",
    "    for term in unique(dataf[did]):\n",
    "        if term not in t_d:\n",
    "            t_d.append(term)\n",
    "            denom1[did] += (dataf[did].count(term))**2\n",
    "        t_d = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e405b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#denom1 = {}\n",
    "for i in denom1.keys():\n",
    "    denom1[i]=math.sqrt(denom1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "denom1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "denom2 = {}\n",
    "t_d = []\n",
    "for qid in q_data:\n",
    "    denom2[qid] = 0\n",
    "    for term in q_data[qid]:\n",
    "        if term not in t_d:\n",
    "            t_d.append(term)\n",
    "            denom2[qid] += (q_data[qid].count(term))**2\n",
    "        t_d = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb04502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#denom2_ = {}\n",
    "for i in denom2:\n",
    "    denom2[i]=math.sqrt(denom2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a15be",
   "metadata": {},
   "outputs": [],
   "source": [
    "denom2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denominator(qid):\n",
    "    denom = {}\n",
    "    for did in denom1:\n",
    "        denom[did] = denom1[did] * denom2[qid]\n",
    "    return denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "denomf = {}\n",
    "for qid in q_data:\n",
    "    denomf[qid] = denominator(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca096f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid in vecnu:\n",
    "    for did in vecnu[qid]:\n",
    "        vecnu[qid][did] = vecnu[qid][did] / denomf[qid][did]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = {}\n",
    "cos = vecnu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5ce9a",
   "metadata": {},
   "source": [
    "### Sorting the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vsm = {}\n",
    "for query in cos.keys():\n",
    "    sorted_vsm[query] = sorted(cos[query].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc25ed0",
   "metadata": {},
   "source": [
    "### Writing the results in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d230c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "with open('outVSM100.txt','w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        for query in sorted_vsm:\n",
    "            count = 1\n",
    "            for i in range(0, len(sorted_vsm[query])):\n",
    "                if (count >= 101):\n",
    "                    break\n",
    "                print(query,'1',sorted_vsm[query][i][0], count, sorted_vsm[query][i][1],'run1')\n",
    "                count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01200da9",
   "metadata": {},
   "source": [
    "### VSM Using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadd96f",
   "metadata": {},
   "source": [
    "### Getting TF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idf={}\n",
    "\n",
    "for word in total_vocabulary:\n",
    "    idf[word] = np.log2(len(dataf)/len(inv_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53464f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed7954",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "for word in total_vocabulary:\n",
    "    for i in range (0, len(inv_index[word])):\n",
    "        #tf_idf[word] = idf[word] * inv_index[word][i][1]\n",
    "        tf_idf.setdefault(word,[]).append((inv_index[word][i][0],idf[word] * inv_index[word][i][1]))\n",
    "    \n",
    "tf_idf['regardless']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r idf\n",
    "%store -r tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b512427",
   "metadata": {},
   "source": [
    "#### Getting the numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simi_n(query):\n",
    "    num_s={}\n",
    "    for term in query:\n",
    "        if term not in tf_idf:\n",
    "            continue\n",
    "        for i in range(0,len(tf_idf[term])):\n",
    "            if tf_idf[term][i][0] not in num_s:\n",
    "                num_s[tf_idf[term][i][0]] = 0\n",
    "            num_s[tf_idf[term][i][0]]+= tf_idf[term][i][1]*query.count(term)    \n",
    "    return num_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c824b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecnu_sim = {}\n",
    "for q_id in q_data:\n",
    "    vecnu_sim[q_id] = simi_n(q_data[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d1(term):\n",
    "    d1={}\n",
    "    for term in tf_idf:\n",
    "        for i in range(0,tf_idf[term]):\n",
    "            d1[tf_idf[term][i][0]]=tf_idf[term][i][1]**2\n",
    "    return d1\n",
    "\n",
    "\n",
    "d1={}\n",
    "for did in dataf:\n",
    "    for term in unique(dataf[did]):\n",
    "        #for i in range(0,len(unique(data3[did]))):\n",
    "        if did not in d1:\n",
    "            d1[did]=0\n",
    "        d1[did]+=((dataf[did].count(term))*np.log2(len(dataf)/((dataf[did].count(term)))))**2\n",
    "        #for term in tf_idf:\n",
    "        \n",
    "for i in d1.keys():\n",
    "    d1[i]=math.sqrt(d1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa699473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denominator1(qid):\n",
    "    denomina = {}\n",
    "    for did in d1:\n",
    "        denomina[did]= d1[did]*denom2[qid]\n",
    "    return denomina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "denomina_final = {} \n",
    "for qid in q_data:\n",
    "    denomina_final[qid] = denominator1(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa040405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denomina_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b606e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity \n",
    "\n",
    "for qid in vecnu_sim:\n",
    "    for did in vecnu_sim[qid]:\n",
    "        vecnu_sim[qid][did] = vecnu_sim[qid][did] / denomina_final[qid][did]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9884fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the cosine similarity in separate dictionary cos1 - for the tfidf approach\n",
    "\n",
    "cos1 = vecnu_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9996bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting each the smilarity scores\n",
    "\n",
    "sim_sort1 = {}\n",
    "for i in cos1.keys():\n",
    "    sim_sort1[i]=(sorted(cos1[i].items(), key=lambda x: x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57543ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "with open('outVSMtf10.txt','w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        for query in sim_sort1:\n",
    "            count = 1\n",
    "            for i in range(0, len(sim_sort1[query])):\n",
    "                if (count >= 11):\n",
    "                    break\n",
    "                print(query,'1',sim_sort1[query][i][0], count, sim_sort1[query][i][1],'run1')\n",
    "                count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a19c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def querry_processing(querry_text):\n",
    "    \n",
    "#     #Removing None_types from the querries.\n",
    "#     querry_text=none_type_clean(querry_text)\n",
    "    \n",
    "#     #Changing the case of the querry_text\n",
    "#     querry_text=querry_text.lower()\n",
    "    \n",
    "#     #Expanding english contractions\n",
    "#     querry_text = expand_contractions(querry_text)\n",
    "    \n",
    "#     #Removing Punctuations\n",
    "#     querry_text = remove_punct(querry_text)\n",
    "    \n",
    "#     #Lemmatisation, pos-tagging and tokenisation\n",
    "#     querry_text = lem(querry_text)\n",
    "    \n",
    "#     #Stop_words removal\n",
    "#     #querry_text = [word for word in querry_text if not word in stopwords]\n",
    "\n",
    "#     return querry_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# querry_text = [word for word in querry_text if not word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d437518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(q_dataframe['title'])):\n",
    "#     q_dataframe['title'][i]=querry_processing(q_dataframe['title'][i])\n",
    "# #     q_data['q_desc'][i]=querry_processing(q_data['q_desc'][i])\n",
    "# #     q_data['q_narr'][i]=querry_processing(q_data['q_narr'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e63880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r q_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5978c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39691555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_data ={key: value for key, value in zip(q_dataframe['query_id'], q_dataframe['title'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8977ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def q_weight(query):\n",
    "#     q_freq = {}\n",
    "#     for i in inv_index.keys():\n",
    "#         # initially adding all the elements into the dictionary and initialising the values as zero\n",
    "#         if i not in q_freq:\n",
    "#             q_freq.update({i: 0})\n",
    " \n",
    "#     for val in query:\n",
    "#         # to get the number of occurrence of each terms\n",
    "#         if val in q_freq:\n",
    "#             q_freq[val] += 1\n",
    "#             # value incremented by one if the term is found in the documents\n",
    " \n",
    "#     for i in q_freq:\n",
    "#         q_freq[i] = q_freq[i] / len(query)\n",
    "#         # term frequency obtained by dividing the number of occurrence of terms by total number of terms in the query\n",
    " \n",
    "#     return q_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8127b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qWeight={}\n",
    "# for query in q_data:\n",
    "#     qWeight[query]=q_weight(q_data[query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store qWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# def similarity_Computation(query_Weight):\n",
    "#     ''' Function to calculate the similarity measure in which the weight of the\n",
    "#     query and the document is multiplied in the numerator and the the weight is\n",
    "#     squared and squareroot is taken the weights of the query and document'''\n",
    " \n",
    "#     numerator = 0\n",
    "#     denomi1 = 0\n",
    "#     denomi2 = 0\n",
    "#     # initialisation of the variables with zero which is needed for computation\n",
    " \n",
    "#     similarity = {}\n",
    "#     # initialisation of dictionary which has the name of document as key and the\n",
    "#     # similarity measure as value\n",
    " \n",
    "#     for terms in inv_index:\n",
    "#         #for terms in data3[document]:\n",
    "#             # cosine similarity is calculated\n",
    "#         for i in range(0,len(tf_idf[terms])):\n",
    "#                 numerator += tf_idf[terms][i][1] * query_Weight[terms]\n",
    "#                 denomi1 += tf_idf[terms][i][1] * tf_idf[terms][i][1]\n",
    "#         denomi2 += query_Weight[terms] * query_Weight[terms]\n",
    "#             # the summation values of the weight is calculated and later they are\n",
    "#             # divided\n",
    " \n",
    "#         if denomi1 != 0 and denomi2 != 0:\n",
    "#             # to avoid the zero division error\n",
    "\n",
    "#             for i in range(0,len(inv_index[terms])):\n",
    "#                     simi = numerator / (math.sqrt(denomi1) * math.sqrt(denomi2))\n",
    "\n",
    "#                     similarity.update({inv_index[terms][i][0]: simi})\n",
    "#             #dictionary is updated\n",
    " \n",
    "#             numerator = 0\n",
    "#             denomi2 = 0\n",
    "#             denomi1 = 0\n",
    "#             # reinitialisation of the variables to zero\n",
    " \n",
    "#     return (similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim = {}\n",
    "# for query in qWeight:\n",
    "#     sim[query] = similarity_Computation(qWeight[query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5852c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim['10.2452/141-AH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim1 = {}\n",
    "# for query in sim.keys():\n",
    "#     sim1[query] = sorted(sim[query].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim1['10.2452/141-AH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store sim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k[0]\n",
    "# sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45263362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from contextlib import redirect_stdout\n",
    "# with open('out.txt','w') as file:\n",
    "#     with redirect_stdout(file):\n",
    "#         for query in sim1:\n",
    "#             count = 1\n",
    "#             for i in range(0, len(sim1[query])):\n",
    "#                 if (count >= 101):\n",
    "#                     break\n",
    "#                 print(query,'1',sim1[query][i][0], count, sim1[query][i][1],'run1')\n",
    "#                 count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0064ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from contextlib import redirect_stdout\n",
    "# with open('out1.txt','w') as file:\n",
    "#     with redirect_stdout(file):\n",
    "#         for query in sim1:\n",
    "#             count = 1\n",
    "#             for i in range(0, len(sim1[query])):\n",
    "#                 if (count >= 11):\n",
    "#                     break\n",
    "#                 print(query,'1',sim1[query][i][0], count, sim1[query][i][1],'run1')\n",
    "#                 count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denom = {}\n",
    "# t_d = []\n",
    "# for did in dataf:\n",
    "#     denom[did] = 0\n",
    "#     for term in dataf[did]:\n",
    "#         if term not in t_d:\n",
    "#             t_d.append(term)\n",
    "#             denom[did] += (dataf[did].count(term))**2\n",
    "#         t_d = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb9950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_t_d = {}\n",
    "# for qid in q_data:\n",
    "#     for i in range(0, len(q_data[qid])):\n",
    "#         if q_data[qid][i] in inv_index:\n",
    "#             q_t_d.setdefault(qid, {}).setdefault(q_data[qid][i], []).append(inv_index[q_data[qid][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058af3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qu = ['oil', 'accident',  'bird']\n",
    "# num = {}\n",
    "# for qid in q_data:\n",
    "#     for query in q_t_d:\n",
    "#         for i in range(0, len(q_t_d[qid][0])):\n",
    "#             num[qid] = \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e99fa",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefca49c",
   "metadata": {},
   "source": [
    "##### Calculating the average document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c149c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = 0\n",
    "for doc in dataf:\n",
    "    addition += len(dataf[doc])\n",
    "avg_l = addition/len(dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f8997",
   "metadata": {},
   "source": [
    "#### BM25 function which can calculate the score of all documents for a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e337da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25 (query):\n",
    "    bm = {}\n",
    "    for term in query:\n",
    "        if term not in inv_index:\n",
    "            continue\n",
    "        for i in range (0, len(inv_index[term])):\n",
    "            if inv_index[term][i][0] not in bm:\n",
    "                bm[inv_index[term][i][0]] = 0\n",
    "            bm[inv_index[term][i][0]] += (idf[term] * ((inv_index[term][i][1] * 2.5)))/((inv_index[term][i][1] + (2 * ((1 - 0.75) + 0.75 * (len(dataf[inv_index[term][i][0]])/avg_l)))))\n",
    "    #print(bm)\n",
    "    return bm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077dafa4",
   "metadata": {},
   "source": [
    "#### Creating a dictionary with query id as keys and value as another dictionary containing document id with their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm2 = {}\n",
    "for q_id in q_data:\n",
    "    bm2[q_id] = BM25(q_data[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bm2['10.2452/141-AH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc8410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BM25 (query):\n",
    "#     for term in query: \n",
    "#         for i in range (0, len(inv_index[term])):\n",
    "#             if inv_index[term][i][0] not in bm:\n",
    "#                 bm[inv_index[term][i][0]] = 0\n",
    "#             bm[inv_index[term][i][0]] = bm[inv_index[term][i][0]] / (inv_index[term][i][1] + (2 * (1 - 0.75 + 0.75) * (len(dataf[inv_index[term][i][0]])/avg_l)))\n",
    "#     #print(bm)\n",
    "#     return bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25(q_data['10.2452/141-AH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inv_index[term][i][1] + (2 * (1 - 0.75 + 0.75) * (len(dataf[inv_index[term][i][0]])/avg_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758298b6",
   "metadata": {},
   "source": [
    "### Sorting the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc43990",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bm2 = {}\n",
    "for query in bm2.keys():\n",
    "    sorted_bm2[query] = sorted(bm2[query].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bm2_1 = {}\n",
    "for query in bm2.keys():\n",
    "    sorted_bm2_1[query] = sorted(bm2[query].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store sorted_bm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc34889",
   "metadata": {},
   "source": [
    "### Saving the results of BM25 in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "with open('outbm100.txt','w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        for query in sorted_bm2:\n",
    "            count = 1\n",
    "            for i in range(0, len(sorted_bm2[query])):\n",
    "                if (count >= 101):\n",
    "                    break\n",
    "                print(query,'1',sorted_bm2[query][i][0], count, sorted_bm2[query][i][1],'run1')\n",
    "                count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af77bb",
   "metadata": {},
   "source": [
    "### Multinomial Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ccabd",
   "metadata": {},
   "source": [
    "##### Total terms in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for doc in dataf:\n",
    "    total += len(dataf[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33ee83",
   "metadata": {},
   "source": [
    "##### Count of term in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_term = {}\n",
    "for term in inv_index:\n",
    "    for i in range(0, len(inv_index[term])):\n",
    "        if term not in total_term:\n",
    "            total_term[term] = 0\n",
    "        total_term[term] += inv_index[term][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077bad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_term['first']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e2b68",
   "metadata": {},
   "source": [
    "#### Function to get the probablity of a given query in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_q(query):\n",
    "    prob = {}\n",
    "    for term in query:\n",
    "        if term not in inv_index.keys():\n",
    "            continue\n",
    "        for i in range (0, len(inv_index[term])):\n",
    "            if inv_index[term][i][0] not in prob:\n",
    "                prob[inv_index[term][i][0]] = 1\n",
    "            if term not in dataf[inv_index[term][i][0]]:\n",
    "                prob[inv_index[term][i][0]] = prob[inv_index[term][i][0]] * (total_term[term] / total)\n",
    "            prob[inv_index[term][i][0]] *= (inv_index[term][i][1] / len(dataf[inv_index[term][i][0]])) #+ (total_term[term] / total)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99090aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = {}\n",
    "for qid in q_data:\n",
    "    prob[qid] = prob_q(q_data[qid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77da2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc437e1",
   "metadata": {},
   "source": [
    "##### Sorting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_prob = {}\n",
    "for query in prob.keys():\n",
    "    sorted_prob[query] = sorted(prob[query].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c721ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16975d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "with open('LM100.txt','w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        for query in sorted_prob:\n",
    "            count = 1\n",
    "            for i in range(0, len(sorted_prob[query])):\n",
    "                if (count >= 101):\n",
    "                    break\n",
    "                print(query,'1',sorted_prob[query][i][0], count, sorted_prob[query][i][1],'run1')\n",
    "                count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store sorted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f111b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
